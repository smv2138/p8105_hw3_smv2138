---
title: "Homework 3"
output: github_document
author: Sushupta Vijapur
---
```{r setup}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = 0.6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

## all plots i make will have the viridis color palette
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```


## Problem 1

Load "instacart" dataset

```{r}
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and ... columns 

Observations are the level of itesm in the order by users. There are user / order variables -- user id, order id, order day and order hour. There are also item variables -- name, aisle, department and some numeric codes.

### Part 1
How many aisles and which are the most items from?

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

### Part 2
Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.
Counting produces a dataframe that we can manipulate

Notes for me: 
We want to rotate axis labels because they're all overlapping 
Order from least to most number of orders (change aisle to a factor)
  "aisle = fct_reorder(aisle, n)" reorder aisle by n
```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

### Part 3
Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.

Notes for me:
To do a ranking that is different for each aisle, we need to group first
Putting group_by before count is important because in the output we will retain the aisle. So it will count the n BY the aisle.
```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(aisle, rank)) %>% 
  knitr::kable()
  
```

### Part 4
Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers

```{r}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```

## Problem 2

### Part 1
Load, tidy, and otherwise wrangle the data. Your final dataset should include all originally observed variables and values; have useful variable names; include a weekday vs weekend variable; and encode data with reasonable variable classes. Describe the resulting dataset (e.g. what variables exist, how many observations, etc).

```{r}
accel_data = 
  read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
   pivot_longer(
    activity_1:activity_1440,
    names_to = "min_of_day",
    names_prefix = "activity_",
    values_to = "activity_count"
  ) %>% 
  mutate(
    activity_count = as.numeric(activity_count),
    min_of_day = as.numeric(min_of_day),
    day_type = ifelse(day == "Saturday" | day == "Sunday", "weekend", "weekday"),
    day = factor(day),
    day = fct_relevel(day, "Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")
  )

```
The results dataframe has `r nrow(accel_data)` rows and `r ncol(accel_data)` columns. The data frame includes week, day_id and day of the week variables. I used the day variable to create a day_type variable which indicates whether it is a weekday or a weekend. Furthermore, I pivoted the activity counts and minutes variables to longer. Now there is an activity count and min of the day variable in tidy format.Furthermore, by releveling the data, the days of the week will be listed in intuitive order. 


### Part 2
Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?

```{r}
accel_data %>% 
  group_by(day, week) %>% 
  summarize(total_act = sum(activity_count)) %>% 
  pivot_wider(
    names_from = day,
    values_from = total_act
  ) %>% 
   knitr::kable()
  

```



### Part 3
Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.

```{r}
accel_data %>% 
  ggplot(aes(x = min_of_day, y = activity_count, color = day)) +
  geom_line() +
  geom_smooth(aes(group = day)) +
  labs(
    title = "Activity Plot",
    x = "Minutes of the day",
    y = "Activity Count"
  )
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

24 hours of activity looks like in each minute in each day
min on x activity count on y. color is day of the week
geom_line might work better than a scatter


## Problem 3

### Part 1
Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?
```{r}
data("ny_noaa")
```

Data cleaning

Notes for me: Separated the date varible. Converted prcp, snow, snwd, tmin and tmax to numeric. Changed prcp, snow and snwd from mm to cm. Changed tmax and tmin from tenth of degrees of C to C. 
```{r}
ny_noaa =
  ny_noaa %>% 
  separate(date, into = c("year", "day", "month")) %>% 
  mutate(
    prcp = as.numeric(prcp*0.1),
    snow = as.numeric(snow*0.1),
    snwd = as.numeric(snwd*0.1),
    tmax = as.numeric(tmax)*10,
    tmin = as.numeric(tmin)*10
  ) %>% 
  count(snow) %>% 
  arrange(desc(n))
```
The most commonly observed value for snowfall in cm is 0. This is probably due to the fact that the data is collected from all around the world, and in many parts of the world, snow is rare or infrequent. Furthermore, the season also impacts how much snow or if there is snow at all in these regions. 



### Part 2
Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

Notes for me:

collection of data mani followed by a plotting step
group by and summarize problem 
groupby station, year, month and the summarize avg max temp
will need to filter for onlt january and july

Part 3
patchwork
one is scatter plot maybe? or contour or hex plot
one is box, violin 